"""
Multi-Modal RAG with Vision Language Model.

This example demonstrates a RAG system that works with both text and images:
1. Index images with descriptions generated by vision model
2. Perform visual question answering
3. Retrieve relevant images and text together
4. Combined multi-modal retrieval and generation

Prerequisites:
- Ollama server running: `ollama serve`
- Models downloaded:
  - `ollama pull qwen3-vl:8b`  (vision-language model)
  - `ollama pull qwen3-embedding`  (for embeddings)
- Directory with images (JPG, PNG, etc.)

Expected output:
Answers to questions about images, with ability to retrieve and reference
specific images from the collection.
"""

import base64
import sys
from pathlib import Path
from typing import List, Dict, Optional
import logging

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama

# Add project root to path for utils imports
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from utils import OllamaManager, VectorStoreManager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VisionRAG:
    """Multi-modal RAG system for images and text."""

    def __init__(
        self,
        vision_model: str = "qwen3-vl:8b",
        base_url: str = "http://localhost:11434"
    ):
        """
        Initialize vision RAG system.

        Args:
            vision_model: Vision-language model name.
            base_url: Ollama API endpoint.
        """
        self.vision_model = vision_model
        self.base_url = base_url
        self.llm = ChatOllama(
            model=vision_model,
            base_url=base_url,
            temperature=0.3
        )
        logger.info(f"Initialized VisionRAG with model: {vision_model}")

    def encode_image(self, image_path: Path) -> str:
        """
        Encode image to base64 string.

        Args:
            image_path: Path to image file.

        Returns:
            Base64 encoded image string.
        """
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode("utf-8")

    def describe_image(self, image_path: Path, prompt: Optional[str] = None) -> str:
        """
        Generate description of an image using vision model.

        Args:
            image_path: Path to image file.
            prompt: Optional custom prompt for description.

        Returns:
            Text description of the image.
        """
        default_prompt = """Describe this image in detail. Include:
1. Main subjects or objects
2. Setting and environment
3. Colors and visual style
4. Any text visible in the image
5. Notable details or features

Be specific and thorough."""

        prompt_text = prompt or default_prompt

        try:
            # Encode image
            image_b64 = self.encode_image(image_path)

            # Create message with image
            message = HumanMessage(
                content=[
                    {"type": "text", "text": prompt_text},
                    {
                        "type": "image_url",
                        "image_url": f"data:image/jpeg;base64,{image_b64}"
                    }
                ]
            )

            # Get description
            response = self.llm.invoke([message])
            description = response.content

            logger.info(f"Generated description for {image_path.name}")
            return description

        except Exception as e:
            logger.error(f"Error describing image {image_path}: {e}")
            return f"Error processing image: {str(e)}"

    def answer_visual_question(self, image_path: Path, question: str) -> str:
        """
        Answer a question about a specific image.

        Args:
            image_path: Path to image file.
            question: Question to answer.

        Returns:
            Answer to the question.
        """
        try:
            image_b64 = self.encode_image(image_path)

            message = HumanMessage(
                content=[
                    {"type": "text", "text": f"Question: {question}\n\nAnswer the question based on this image."},
                    {
                        "type": "image_url",
                        "image_url": f"data:image/jpeg;base64,{image_b64}"
                    }
                ]
            )

            response = self.llm.invoke([message])
            logger.info(f"Answered question about {image_path.name}")
            return response.content

        except Exception as e:
            logger.error(f"Error answering question for {image_path}: {e}")
            return f"Error: {str(e)}"


def index_images(
    image_dir: Path,
    vision_rag: VisionRAG,
    vector_mgr: VectorStoreManager,
    collection_name: str,
    persist_dir: str,
    rebuild: bool = False
) -> tuple:
    """
    Index images with generated descriptions.

    Args:
        image_dir: Directory containing images.
        vision_rag: VisionRAG instance.
        vector_mgr: VectorStoreManager instance.
        collection_name: Collection name for vector store.
        persist_dir: Directory to persist vector store.
        rebuild: Whether to rebuild existing index.

    Returns:
        Tuple of (vectorstore, image_documents).
    """
    # Check if collection exists
    collections = vector_mgr.list_collections(persist_dir)
    collection_exists = collection_name in collections.get("chroma", [])

    if collection_exists and not rebuild:
        logger.info(f"Loading existing collection: {collection_name}")
        vectorstore = vector_mgr.load_existing(
            collection_name=collection_name,
            persist_dir=persist_dir
        )
        # Load metadata from collection
        # Note: In production, you'd want to persist the image paths separately
        image_docs = []
        return vectorstore, image_docs

    logger.info("Creating new image index...")

    # Find all images
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}
    image_files = [
        f for f in image_dir.iterdir()
        if f.is_file() and f.suffix.lower() in image_extensions
    ]

    logger.info(f"Found {len(image_files)} images")

    if not image_files:
        raise ValueError(f"No images found in {image_dir}")

    # Generate descriptions for each image
    documents = []
    for i, image_path in enumerate(image_files, 1):
        logger.info(f"Processing image {i}/{len(image_files)}: {image_path.name}")

        try:
            description = vision_rag.describe_image(image_path)

            # Create document with image metadata
            doc = Document(
                page_content=description,
                metadata={
                    'source': str(image_path),
                    'filename': image_path.name,
                    'type': 'image',
                    'format': image_path.suffix[1:].lower()
                }
            )
            documents.append(doc)

        except Exception as e:
            logger.error(f"Failed to process {image_path.name}: {e}")
            continue

    if not documents:
        raise ValueError("Failed to process any images")

    logger.info(f"Successfully processed {len(documents)} images")

    # Create vector store
    logger.info("Creating vector store...")
    vectorstore = vector_mgr.create_from_documents(
        documents=documents,
        collection_name=collection_name,
        persist_dir=persist_dir
    )

    return vectorstore, documents


def create_multimodal_qa(
    vectorstore,
    vision_rag: VisionRAG,
    text_model: str = "qwen3:8b"
):
    """
    Create multi-modal QA system.

    Args:
        vectorstore: Vector store with image descriptions.
        vision_rag: VisionRAG instance.
        text_model: Text-only model for reasoning.

    Returns:
        QA function.
    """
    text_llm = ChatOllama(
        model=text_model,
        temperature=0.2,
        base_url="http://localhost:11434"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful assistant that answers questions about a collection of images.
You have access to descriptions of images. Use these descriptions to answer questions.

Image Descriptions:
{context}"""),
        ("human", "{question}")
    ])

    def qa_function(query: str, k: int = 3) -> dict:
        """
        Answer questions about images.

        Args:
            query: Question to answer.
            k: Number of images to retrieve.

        Returns:
            Dictionary with answer and source images.
        """
        # Retrieve relevant image descriptions
        logger.info(f"Searching for relevant images: '{query}'")
        docs = vectorstore.similarity_search(query, k=k)

        if not docs:
            return {
                'answer': "No relevant images found.",
                'images': [],
                'mode': 'none'
            }

        # Check if query asks for visual analysis
        visual_keywords = ['show', 'see', 'look', 'picture', 'visual', 'color', 'shape']
        needs_visual = any(keyword in query.lower() for keyword in visual_keywords)

        if needs_visual and docs:
            # Use vision model to answer with the most relevant image
            top_image_path = Path(docs[0].metadata['source'])
            logger.info(f"Using vision model for image: {top_image_path.name}")

            answer = vision_rag.answer_visual_question(top_image_path, query)

            return {
                'answer': answer,
                'images': docs,
                'mode': 'vision',
                'primary_image': str(top_image_path)
            }
        else:
            # Use text model with descriptions
            context = "\n\n".join([
                f"Image: {doc.metadata.get('filename', 'unknown')}\n{doc.page_content}"
                for doc in docs
            ])

            chain = prompt | text_llm
            response = chain.invoke({"context": context, "question": query})

            return {
                'answer': response.content,
                'images': docs,
                'mode': 'text'
            }

    return qa_function


def interactive_visual_qa_loop(qa_function):
    """
    Run interactive visual Q&A session.

    Args:
        qa_function: QA function to use.
    """
    print("\n" + "=" * 80)
    print("Multi-Modal Vision RAG System")
    print("=" * 80)
    print("Ask questions about the images. Type 'quit' to exit.")
    print("Prefix questions with 'show:' to get visual analysis of specific images.")
    print("=" * 80 + "\n")

    while True:
        try:
            question = input("\nQuestion: ").strip()

            if not question:
                continue

            if question.lower() in ["quit", "exit", "q"]:
                print("\nGoodbye!")
                break

            # Query
            print("\nProcessing...")
            result = qa_function(question)

            # Display answer
            print("\nAnswer:")
            print("-" * 60)
            print(result['answer'])
            print("-" * 60)

            # Show source images
            if result['images']:
                print(f"\nRelevant images ({len(result['images'])}):")
                for i, doc in enumerate(result['images'], 1):
                    filename = doc.metadata.get('filename', 'unknown')
                    print(f"  {i}. {filename}")

                if 'primary_image' in result:
                    print(f"\nPrimary image analyzed: {Path(result['primary_image']).name}")

            print(f"\n[Mode: {result['mode']}]")

        except KeyboardInterrupt:
            print("\n\nGoodbye!")
            break
        except Exception as e:
            logger.error(f"Error: {e}", exc_info=True)
            print(f"\nError: {e}")


def main():
    """Main execution function."""
    import argparse

    parser = argparse.ArgumentParser(description="Multi-modal Vision RAG system")
    parser.add_argument(
        "image_dir",
        help="Directory containing images"
    )
    parser.add_argument(
        "--collection",
        default="vision_rag",
        help="Collection name (default: vision_rag)"
    )
    parser.add_argument(
        "--persist-dir",
        default="./data/vector_stores",
        help="Vector store directory (default: ./data/vector_stores)"
    )
    parser.add_argument(
        "--vision-model",
        default="qwen3-vl:8b",
        help="Vision model (default: qwen3-vl:8b)"
    )
    parser.add_argument(
        "--text-model",
        default="qwen3:8b",
        help="Text model (default: qwen3:8b)"
    )
    parser.add_argument(
        "--embedding-model",
        default="qwen3-embedding",
        help="Embedding model (default: qwen3-embedding)"
    )
    parser.add_argument(
        "--rebuild",
        action="store_true",
        help="Rebuild image index"
    )

    args = parser.parse_args()

    print("=" * 80)
    print("Vision RAG System - Initialization")
    print("=" * 80)

    # Verify image directory
    image_dir = Path(args.image_dir)
    if not image_dir.exists():
        print(f"Error: Image directory not found: {args.image_dir}")
        return

    # Step 1: Check Ollama
    print("\n1. Checking Ollama...")
    ollama_mgr = OllamaManager()

    if not ollama_mgr.check_ollama_running():
        print("Error: Ollama not running! Start with: ollama serve")
        return

    # Step 2: Verify models
    print("\n2. Verifying models...")
    required_models = [args.vision_model, args.text_model, args.embedding_model]
    for model_name in required_models:
        print(f"   - {model_name}")
        if not ollama_mgr.ensure_model_available(model_name):
            print(f"Error: Model '{model_name}' not available")
            print(f"Install with: ollama pull {model_name}")
            return

    # Step 3: Initialize components
    print("\n3. Initializing vision RAG...")
    vision_rag = VisionRAG(vision_model=args.vision_model)
    vector_mgr = VectorStoreManager(embedding_model=args.embedding_model)

    # Step 4: Index images
    print("\n4. Indexing images...")
    print(f"   Source: {image_dir}")
    try:
        vectorstore, image_docs = index_images(
            image_dir=image_dir,
            vision_rag=vision_rag,
            vector_mgr=vector_mgr,
            collection_name=args.collection,
            persist_dir=args.persist_dir,
            rebuild=args.rebuild
        )
    except Exception as e:
        print(f"Error indexing images: {e}")
        return

    # Step 5: Create QA system
    print("\n5. Creating QA system...")
    qa_function = create_multimodal_qa(
        vectorstore=vectorstore,
        vision_rag=vision_rag,
        text_model=args.text_model
    )

    # Step 6: Start interactive session
    print("\n6. Starting interactive session...")
    interactive_visual_qa_loop(qa_function)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        print(f"\nFatal error: {e}")
        sys.exit(1)
