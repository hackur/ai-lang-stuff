{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Pattern Visualization with TransformerLens\n",
    "\n",
    "This notebook demonstrates how to visualize attention patterns in transformer models using TransformerLens.\n",
    "\n",
    "## What You'll Learn\n",
    "- Loading models with TransformerLens\n",
    "- Extracting attention patterns\n",
    "- Visualizing attention heads\n",
    "- Identifying induction heads\n",
    "- Understanding model behavior\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "uv add transformer-lens plotly circuitsvis\n",
    "```\n",
    "\n",
    "## References\n",
    "- [TransformerLens Docs](https://transformerlensorg.github.io/TransformerLens/)\n",
    "- [Anthropic's Circuits Thread](https://transformer-circuits.pub/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup and imports.\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from transformer_lens import HookedTransformer\n",
    "import circuitsvis as cv\n",
    "\n",
    "# Set device\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load a Small Transformer Model\n",
    "\n",
    "We'll use GPT-2 Small for this demonstration. TransformerLens provides easy access to various pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load model with TransformerLens.\"\"\"\n",
    "\n",
    "# Load GPT-2 Small (124M parameters)\n",
    "# TransformerLens adds hooks for easy activation access\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\", center_unembed=True, center_writing_weights=True, fold_ln=True, device=device\n",
    ")\n",
    "\n",
    "print(f\"Model: {model.cfg.model_name}\")\n",
    "print(f\"Layers: {model.cfg.n_layers}\")\n",
    "print(f\"Attention heads: {model.cfg.n_heads}\")\n",
    "print(f\"Hidden dimension: {model.cfg.d_model}\")\n",
    "print(f\"Vocabulary size: {model.cfg.d_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Text and Capture Attention\n",
    "\n",
    "We'll run a prompt through the model and capture all attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run inference and capture attention patterns.\"\"\"\n",
    "\n",
    "# Test prompt with repeated pattern to trigger induction heads\n",
    "prompt = \"When Mary and John went to the store, Mary gave a drink to John. Then Mary and John went to the park, Mary gave a ball to\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = model.to_tokens(prompt)\n",
    "print(f\"Tokens: {tokens.shape}\")\n",
    "print(f\"Token IDs: {tokens[0][:20]}...\")  # Show first 20\n",
    "\n",
    "# Run with caching to capture all activations\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "# Get attention patterns: shape [batch, layer, head, query_pos, key_pos]\n",
    "attention_patterns = cache[\"pattern\"]\n",
    "print(f\"\\nAttention patterns shape: {attention_patterns.shape}\")\n",
    "print(f\"  Batch: {attention_patterns.shape[0]}\")\n",
    "print(f\"  Layers: {attention_patterns.shape[1]}\")\n",
    "print(f\"  Heads: {attention_patterns.shape[2]}\")\n",
    "print(f\"  Sequence length: {attention_patterns.shape[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Attention Patterns for a Single Head\n",
    "\n",
    "Let's examine how a specific attention head attends to different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize attention for a specific layer and head.\"\"\"\n",
    "\n",
    "\n",
    "def visualize_attention_head(attention_pattern, tokens, layer_idx, head_idx, model):\n",
    "    \"\"\"\n",
    "    Create an interactive heatmap of attention patterns.\n",
    "\n",
    "    Args:\n",
    "        attention_pattern: Attention weights [query_pos, key_pos]\n",
    "        tokens: Token IDs\n",
    "        layer_idx: Layer index\n",
    "        head_idx: Head index\n",
    "        model: HookedTransformer model for token decoding\n",
    "    \"\"\"\n",
    "    # Get attention for this head\n",
    "    attn = attention_pattern[0, layer_idx, head_idx].cpu().numpy()\n",
    "\n",
    "    # Decode tokens to strings\n",
    "    str_tokens = [model.to_string(t) for t in tokens[0]]\n",
    "\n",
    "    # Create heatmap\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=attn,\n",
    "            x=str_tokens,\n",
    "            y=str_tokens,\n",
    "            colorscale=\"Blues\",\n",
    "            hoverongaps=False,\n",
    "            hovertemplate=\"Query: %{y}<br>Key: %{x}<br>Attention: %{z:.3f}<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Attention Pattern - Layer {layer_idx}, Head {head_idx}\",\n",
    "        xaxis_title=\"Key Position (attending to)\",\n",
    "        yaxis_title=\"Query Position (attending from)\",\n",
    "        width=800,\n",
    "        height=800,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Visualize Layer 5, Head 1 (often an induction head in GPT-2)\n",
    "fig = visualize_attention_head(attention_patterns, tokens, layer_idx=5, head_idx=1, model=model)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Find Induction Heads\n",
    "\n",
    "Induction heads are attention heads that complete repeated patterns. They attend to tokens that previously followed the current context.\n",
    "\n",
    "### How Induction Heads Work\n",
    "Given: \"A B ... A\" â†’ Predict \"B\"\n",
    "- The head attends back to the previous occurrence of the current token\n",
    "- Then copies the token that came after it\n",
    "\n",
    "We'll use a specialized test to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Detect induction heads using repeated sequence test.\"\"\"\n",
    "\n",
    "\n",
    "def test_induction_heads(model, seq_len=50):\n",
    "    \"\"\"\n",
    "    Test for induction heads using random repeated sequences.\n",
    "\n",
    "    Induction heads should show high attention to [A][B] when seeing [A] again.\n",
    "\n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        seq_len: Length of test sequence\n",
    "\n",
    "    Returns:\n",
    "        Induction scores per head [layer, head]\n",
    "    \"\"\"\n",
    "    # Create repeated random sequence: [a, b, c, ..., a, b, c, ...]\n",
    "    half_len = seq_len // 2\n",
    "    random_seq = torch.randint(100, 1000, (half_len,), device=device)\n",
    "    repeated_seq = torch.cat([random_seq, random_seq]).unsqueeze(0)\n",
    "\n",
    "    # Run through model\n",
    "    logits, cache = model.run_with_cache(repeated_seq)\n",
    "    attention = cache[\"pattern\"]\n",
    "\n",
    "    # For each position in second half, check if it attends to matching token in first half\n",
    "    # Induction score: attention from pos i+half_len to pos i+1\n",
    "    induction_scores = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            # Get attention in second half\n",
    "            attn = attention[0, layer, head, half_len:, :]\n",
    "\n",
    "            # For each query position in second half,\n",
    "            # check attention to the token that came after the match in first half\n",
    "            induction_attn = []\n",
    "            for i in range(half_len - 1):\n",
    "                query_pos = i\n",
    "                # Key position should be i+1 (token after the match)\n",
    "                key_pos = i + 1\n",
    "                induction_attn.append(attn[query_pos, key_pos].item())\n",
    "\n",
    "            induction_scores[layer, head] = np.mean(induction_attn)\n",
    "\n",
    "    return induction_scores\n",
    "\n",
    "\n",
    "# Test for induction heads\n",
    "induction_scores = test_induction_heads(model)\n",
    "\n",
    "# Find top induction heads\n",
    "top_k = 5\n",
    "flat_scores = induction_scores.flatten()\n",
    "top_indices = torch.topk(flat_scores, top_k).indices\n",
    "\n",
    "print(\"Top Induction Heads:\")\n",
    "print(\"=\" * 50)\n",
    "for idx in top_indices:\n",
    "    layer = idx // model.cfg.n_heads\n",
    "    head = idx % model.cfg.n_heads\n",
    "    score = induction_scores[layer, head]\n",
    "    print(f\"Layer {layer:2d}, Head {head:2d}: Score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize All Attention Heads\n",
    "\n",
    "Create a grid view of all attention heads to compare patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create grid visualization of attention heads.\"\"\"\n",
    "\n",
    "\n",
    "def visualize_all_heads_in_layer(attention_pattern, tokens, layer_idx, model, max_seq_len=30):\n",
    "    \"\"\"\n",
    "    Visualize all attention heads in a layer as a grid.\n",
    "\n",
    "    Args:\n",
    "        attention_pattern: Full attention patterns\n",
    "        tokens: Token IDs\n",
    "        layer_idx: Which layer to visualize\n",
    "        model: HookedTransformer model\n",
    "        max_seq_len: Limit sequence length for readability\n",
    "    \"\"\"\n",
    "    n_heads = model.cfg.n_heads\n",
    "\n",
    "    # Truncate sequence if too long\n",
    "    seq_len = min(max_seq_len, tokens.shape[1])\n",
    "    tokens[:, :seq_len]\n",
    "\n",
    "    # Create subplots\n",
    "    rows = 3\n",
    "    cols = 4  # 12 heads in GPT-2 small\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=[f\"Head {i}\" for i in range(n_heads)],\n",
    "        vertical_spacing=0.05,\n",
    "        horizontal_spacing=0.05,\n",
    "    )\n",
    "\n",
    "    # Add heatmap for each head\n",
    "    for head_idx in range(n_heads):\n",
    "        row = head_idx // cols + 1\n",
    "        col = head_idx % cols + 1\n",
    "\n",
    "        attn = attention_pattern[0, layer_idx, head_idx, :seq_len, :seq_len].cpu().numpy()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=attn,\n",
    "                colorscale=\"Blues\",\n",
    "                showscale=(head_idx == n_heads - 1),  # Only show scale on last plot\n",
    "                hoverongaps=False,\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"All Attention Heads - Layer {layer_idx}\", height=800, width=1200, showlegend=False\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Visualize all heads in layer 5\n",
    "fig = visualize_all_heads_in_layer(attention_patterns, tokens, layer_idx=5, model=model)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Attention Pattern Statistics\n",
    "\n",
    "Analyze quantitative properties of attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute attention pattern statistics.\"\"\"\n",
    "\n",
    "\n",
    "def analyze_attention_statistics(attention_patterns, model):\n",
    "    \"\"\"\n",
    "    Compute various statistics about attention patterns.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of statistics per head\n",
    "    \"\"\"\n",
    "    n_layers = model.cfg.n_layers\n",
    "    n_heads = model.cfg.n_heads\n",
    "\n",
    "    stats = {\n",
    "        \"entropy\": torch.zeros(n_layers, n_heads),\n",
    "        \"max_attention\": torch.zeros(n_layers, n_heads),\n",
    "        \"mean_distance\": torch.zeros(n_layers, n_heads),\n",
    "    }\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        for head in range(n_heads):\n",
    "            attn = attention_patterns[0, layer, head]\n",
    "\n",
    "            # Entropy: how spread out is attention?\n",
    "            # High entropy = diffuse attention, low = focused\n",
    "            entropy = -(attn * torch.log(attn + 1e-10)).sum(dim=-1).mean()\n",
    "            stats[\"entropy\"][layer, head] = entropy\n",
    "\n",
    "            # Max attention: what's the peak attention value?\n",
    "            stats[\"max_attention\"][layer, head] = attn.max()\n",
    "\n",
    "            # Mean distance: how far back does this head look?\n",
    "            seq_len = attn.shape[0]\n",
    "            distances = torch.arange(seq_len, device=attn.device).unsqueeze(0) - torch.arange(\n",
    "                seq_len, device=attn.device\n",
    "            ).unsqueeze(1)\n",
    "            distances = distances.float().abs()\n",
    "            mean_dist = (attn * distances).sum(dim=-1).mean()\n",
    "            stats[\"mean_distance\"][layer, head] = mean_dist\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Compute statistics\n",
    "stats = analyze_attention_statistics(attention_patterns, model)\n",
    "\n",
    "# Visualize entropy across all heads\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=stats[\"entropy\"].numpy(),\n",
    "        x=[f\"Head {i}\" for i in range(model.cfg.n_heads)],\n",
    "        y=[f\"Layer {i}\" for i in range(model.cfg.n_layers)],\n",
    "        colorscale=\"Viridis\",\n",
    "        colorbar_title=\"Entropy\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Attention Entropy by Head (Lower = More Focused)\",\n",
    "    xaxis_title=\"Head\",\n",
    "    yaxis_title=\"Layer\",\n",
    "    height=600,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nAttention Pattern Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(\n",
    "    f\"Most focused head: L{stats['entropy'].argmin() // model.cfg.n_heads}, \"\n",
    "    f\"H{stats['entropy'].argmin() % model.cfg.n_heads} \"\n",
    "    f\"(entropy: {stats['entropy'].min():.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"Most diffuse head: L{stats['entropy'].argmax() // model.cfg.n_heads}, \"\n",
    "    f\"H{stats['entropy'].argmax() % model.cfg.n_heads} \"\n",
    "    f\"(entropy: {stats['entropy'].max():.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nLongest-range head: L{stats['mean_distance'].argmax() // model.cfg.n_heads}, \"\n",
    "    f\"H{stats['mean_distance'].argmax() % model.cfg.n_heads} \"\n",
    "    f\"(mean distance: {stats['mean_distance'].max():.2f} tokens)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Interactive Attention Visualization with CircuitsVis\n",
    "\n",
    "Use CircuitsVis for a more polished, interactive visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use CircuitsVis for interactive attention visualization.\"\"\"\n",
    "\n",
    "# Get string tokens\n",
    "str_tokens = [model.to_string(t) for t in tokens[0]]\n",
    "\n",
    "# CircuitsVis expects attention in shape [n_heads, query_pos, key_pos]\n",
    "# Let's visualize layer 5\n",
    "layer_attn = attention_patterns[0, 5]  # [n_heads, seq_len, seq_len]\n",
    "\n",
    "# Create interactive visualization\n",
    "cv.attention.attention_patterns(\n",
    "    tokens=str_tokens,\n",
    "    attention=layer_attn.cpu().numpy(),\n",
    "    attention_head_names=[f\"Head {i}\" for i in range(model.cfg.n_heads)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "1. **Induction Heads** (typically in middle layers)\n",
    "   - Attend to previous occurrences of current token\n",
    "   - Enable in-context learning\n",
    "   - Show diagonal stripe patterns in repeated sequences\n",
    "\n",
    "2. **Previous Token Heads** (early layers)\n",
    "   - Attend primarily to the previous token\n",
    "   - Help with local syntax and n-gram patterns\n",
    "   - Show strong diagonal patterns\n",
    "\n",
    "3. **Positional Heads**\n",
    "   - Attend based on absolute or relative position\n",
    "   - Often attend to special tokens (BOS, separator)\n",
    "   - May show structured geometric patterns\n",
    "\n",
    "4. **Syntax Heads**\n",
    "   - Attend to syntactically related tokens\n",
    "   - Subject-verb agreement, noun-modifier relations\n",
    "   - More complex, context-dependent patterns\n",
    "\n",
    "### Next Steps:\n",
    "- Try different prompts and analyze how attention changes\n",
    "- Examine how induction heads behave with different repetition patterns\n",
    "- Investigate which heads are important for specific tasks\n",
    "- Move on to activation patching to test causal relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Model Download Issues\n",
    "If model download fails:\n",
    "```python\n",
    "# Try with different cache directory\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/path/to/cache'\n",
    "model = HookedTransformer.from_pretrained('gpt2-small')\n",
    "```\n",
    "\n",
    "### Memory Issues\n",
    "If running out of memory:\n",
    "- Use shorter sequences (truncate tokens)\n",
    "- Clear cache between runs: `torch.cuda.empty_cache()`\n",
    "- Use smaller model: 'gpt2-small' instead of 'gpt2-medium'\n",
    "\n",
    "### Visualization Not Showing\n",
    "If plots don't appear:\n",
    "- Ensure Jupyter is running in notebook mode\n",
    "- Try `fig.show(renderer='browser')`\n",
    "- Check plotly installation: `uv add plotly`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
