# Local AI Development - Environment Variables
# Copy this file to .env and fill in your values

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=120

# LM Studio (alternative to Ollama)
LMSTUDIO_ENABLED=false
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# LangSmith (optional - for observability)
LANGCHAIN_TRACING_V2=false
LANGCHAIN_PROJECT=local-ai-experiments
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
# LANGCHAIN_API_KEY=your-key-here

# MCP Servers
MCP_GITHUB_TOKEN=
MCP_POSTGRES_URL=
MCP_FILESYSTEM_ROOT=./

# API Keys (if using external services)
# OPENAI_API_KEY=
# ANTHROPIC_API_KEY=

# Database
DATABASE_URL=sqlite:///./data/app.db

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/app.log

# Performance
MAX_CONCURRENT_REQUESTS=5
CACHE_ENABLED=true
CACHE_TTL=3600

# Model Settings
DEFAULT_MODEL=qwen3:8b
DEFAULT_TEMPERATURE=0.7
MAX_TOKENS=4096

# Development
DEBUG=false
VERBOSE=false
